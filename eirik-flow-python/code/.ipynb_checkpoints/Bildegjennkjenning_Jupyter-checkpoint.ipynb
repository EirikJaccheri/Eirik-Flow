{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuralt Nettverk for bildegjennkjennign\n",
    "Eirik Høydalsvik\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivasjon\n",
    "Etter flere år med eksponering for AI igjennom media ble jeg innspirert til å lære mer om hvordan AI fungerer og implementere et neuralt nettverk. Prosjektet er basert på en videoserie av youtubekanalen 3Blue1Brown som forklarer matematikken bak neurale nettverk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,urllib.request\n",
    "import codecs\n",
    "import gzip\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nedlasting\n",
    "Minst datasettet er lagret med IDX filformatet som jeg var ukjent med før jeg startet prosjektet. Baserte meg på en kode laget av Ghosh4AI på youtube: https://www.youtube.com/watch?v=6xar6bxD80g og bruker biblioteket pickle for å lagre datasettene. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-images-idx3-ubyte.gz  already exists\n",
      "train-labels-idx1-ubyte.gz  already exists\n",
      "t10k-images-idx3-ubyte.gz  already exists\n",
      "t10k-labels-idx1-ubyte.gz  already exists\n",
      "All files are available\n",
      "Extracting  t10k-images-idx3-ubyte.gz\n",
      "Extracting  t10k-labels-idx1-ubyte.gz\n",
      "Extracting  train-images-idx3-ubyte.gz\n",
      "Extracting  train-labels-idx1-ubyte.gz\n",
      "Extraction Complete\n",
      "Reading  t10k-images-idx3-ubyte\n",
      "Reading  t10k-labels-idx1-ubyte\n",
      "Reading  train-images-idx3-ubyte\n",
      "Reading  train-labels-idx1-ubyte\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PROVIDE YOUR DOWNLOAD DIRECTORY HERE\n",
    "datapath = '/../Data/'\n",
    "\n",
    "# CREATING DOWNLOAD DIRECTORY\n",
    "if not os.path.exists(datapath):\n",
    "    os.makedirs(datapath)\n",
    "\n",
    "# URLS TO DOWNLOAD FROM\n",
    "urls = ['http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "for url in urls:\n",
    "    filename = url.split('/')[-1]   # GET FILENAME\n",
    "    \n",
    "    if os.path.exists(datapath+filename):\n",
    "        print(filename, ' already exists')  # CHECK IF FILE EXISTS\n",
    "    else:\n",
    "        print('Downloading ',filename)\n",
    "        urllib.request.urlretrieve (url, datapath+filename) # DOWNLOAD FILE\n",
    "     \n",
    "print('All files are available')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LISTING ALL ARCHIVES IN THE DIRECTORY\n",
    "files = os.listdir(datapath)\n",
    "for file in files:\n",
    "    if file.endswith('gz'):\n",
    "        print('Extracting ',file)\n",
    "        with gzip.open(datapath+file, 'rb') as f_in:\n",
    "            with open(datapath+file.split('.')[0], 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "print('Extraction Complete')\n",
    "\n",
    "files = os.listdir(datapath)\n",
    "def get_int(b):   # CONVERTS 4 BYTES TO A INT\n",
    "    return int(codecs.encode(b, 'hex'), 16)\n",
    "\n",
    "data_dict = {}\n",
    "for file in files:\n",
    "    if file.endswith('ubyte'):  # FOR ALL 'ubyte' FILES\n",
    "        print('Reading ',file)\n",
    "        with open (datapath+file,'rb') as f:\n",
    "            data = f.read()\n",
    "            type = get_int(data[:4])   # 0-3: THE MAGIC NUMBER TO WHETHER IMAGE OR LABEL\n",
    "            length = get_int(data[4:8])  # 4-7: LENGTH OF THE ARRAY  (DIMENSION 0)\n",
    "            if (type == 2051):\n",
    "                category = 'images'\n",
    "                num_rows = get_int(data[8:12])  # NUMBER OF ROWS  (DIMENSION 1)\n",
    "                num_cols = get_int(data[12:16])  # NUMBER OF COLUMNS  (DIMENSION 2)\n",
    "                parsed = np.frombuffer(data,dtype = np.uint8, offset = 16)  # READ THE PIXEL VALUES AS INTEGERS\n",
    "                parsed = parsed.reshape(length,num_rows*num_cols)  # RESHAPE THE ARRAY AS [NO_OF_SAMPLES x HEIGHT x WIDTH]           \n",
    "            elif(type == 2049):\n",
    "                category = 'labels'\n",
    "                temp = np.frombuffer(data, dtype=np.uint8, offset=8) # READ THE LABEL VALUES AS INTEGERS\n",
    "                temp = temp.reshape(length)  # RESHAPE THE ARRAY AS [NO_OF_SAMPLES]\n",
    "                parsed = np.zeros((len(temp),10))\n",
    "                for i in range(len(temp)):\n",
    "                    parsed[i][temp[i]] = 1\n",
    "            if (length==10000):\n",
    "                set = 'test'\n",
    "            elif (length==60000):\n",
    "                set = 'train'\n",
    "            data_dict[set+'_'+category] = parsed  # SAVE THE np ARRAY TO A CORRESPONDING KEY\n",
    "\n",
    "\n",
    "with open(datapath+'MNISTData.pkl', 'wb') as fp :\n",
    "    pickle.dump(data_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kode\n",
    "Matematikken som ble presentert i videoene til 3Blue1Brown var skrevet for å være pedagogisk og ikke for å være lett å gjøre om til kode. Den første delen av prosjektet gikk dermed ut på å sitte med penn og papir og finne ut hvordan man implementerer gradient descent med numpy matriser (train() funksjonen). Denne delen av prosjektet var krevende og interesant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 0\n",
      "andel av bilder gjettet riktig: 0.2617\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datapath = '/../Data/'\n",
    "with open(datapath+'MNISTData.pkl', 'rb') as fp :\n",
    "    data_dict = pickle.load(fp)\n",
    "\n",
    "#constants\n",
    "h = .1\n",
    "n = 100\n",
    "randOffset = 0.5\n",
    "randRange  = 1.\n",
    "picSize = len(data_dict['train_images'][0])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#returns random weights and biaces \n",
    "def randWeights(shapeArr):\n",
    "    wArr = []\n",
    "    bVec = []\n",
    "    for i in range(len(shapeArr)-1):\n",
    "        wArr.append((np.random.rand(shapeArr[i+1], shapeArr[i]) - randOffset) * randRange) \n",
    "        bVec.append((np.random.rand(shapeArr[i+1]) - randOffset) * randRange)\n",
    "    return wArr,bVec\n",
    "\n",
    "#returns two lists of zero numpy arrays in the shape given by shapeArr\n",
    "def zeroNabla(shapeArr):\n",
    "    wArr = []\n",
    "    bVec = []\n",
    "    for i in range(len(shapeArr)-1):\n",
    "        wArr.append(np.zeros((shapeArr[i+1], shapeArr[i])))\n",
    "        bVec.append((np.zeros(shapeArr[i+1])))\n",
    "    return wArr,bVec\n",
    "\n",
    "#returns the guess given by the weights and biaces wArr and bVec\n",
    "def guess(N,wArr,bVec,label):\n",
    "    a = data_dict[label+'_images'][N]\n",
    "    for i in range(len(wArr)):\n",
    "        a = sigmoid(wArr[i] @ a + bVec[i])\n",
    "    return a\n",
    "\n",
    "#saves the weights and biaces\n",
    "def save(wArr,bVec):\n",
    "    pDict = {}\n",
    "    pDict['w'] = wArr\n",
    "    pDict['b'] = bVec\n",
    "    with open(datapath+'pDict.pkl', 'wb') as fp :\n",
    "        pickle.dump(pDict, fp)\n",
    "\n",
    "#loads the weights and biaces\n",
    "def load():\n",
    "    with open(datapath+'pDict.pkl', 'rb') as fp :\n",
    "        pDict = pickle.load(fp)\n",
    "    return pDict['w'],pDict['b']\n",
    "\n",
    "#tests given weights and biaces wArr and bVec on the 10 000 test_images and returns the proportion that where guessed correctly\n",
    "def test(wArr,bVec):\n",
    "    data = data_dict['test_images']\n",
    "    labels = data_dict['test_labels']\n",
    "    correct = 0\n",
    "    for i in range(len(data)):\n",
    "        c = np.argmax(labels[i])\n",
    "        g = np.argmax(guess(i,wArr,bVec,'test'))\n",
    "        if g == c:\n",
    "            correct +=1\n",
    "    return correct/len(data)\n",
    "\n",
    "#trains the given weights and biaces wArr and bVec on the train_images data set. Passes through the dataset N times\n",
    "def train(wArr,bVec,shapeArr,N):\n",
    "    data = data_dict['train_images']\n",
    "    cost = data_dict['train_labels']\n",
    "\n",
    "    wNabla, bNabla = zeroNabla(shapeArr)\n",
    "\n",
    "    a = []\n",
    "    a.append(data[0])\n",
    "    for i in range(len(wArr)):\n",
    "        a.append(sigmoid(wArr[i] @ a[i] + bVec[i]))\n",
    "    \n",
    "    count = 0\n",
    "    trained = False\n",
    "    while not trained:\n",
    "        print(\"count\",count)\n",
    "        for i in range(len(data)):\n",
    "            a[0] = data[i]\n",
    "            for j in range(0,len(wArr)):\n",
    "                a[j+1] = sigmoid(wArr[j] @ a[j] + bVec[j])\n",
    "\n",
    "            c = 2 * (a[len(wArr)] - cost[i])\n",
    "            for j in range(len(wArr),0,-1):\n",
    "                wNabla[j - 1]  += np.outer(a[j]*(1-a[j]) * c, a[j-1] )\n",
    "                bNabla[j - 1]  += a[j] * (1 - a[j]) * c\n",
    "                c =  wArr[j - 1].T @ ((a[j] * (1-a[j]))*c)\n",
    "\n",
    "            if i % n == 0:\n",
    "                for i in range(len(wArr)):\n",
    "                    wArr[i] -= h *  wNabla[i]/n\n",
    "                    bVec[i] -= h *  bNabla[i]/n\n",
    "                wNabla, bNabla = zeroNabla(shapeArr)\n",
    "        count += 1\n",
    "\n",
    "        if count == N:\n",
    "            trained = True\n",
    "\n",
    "shapeArr = [picSize,16,16,10]\n",
    "wArr, bVec = randWeights(shapeArr)\n",
    "train(wArr,bVec,shapeArr,1)\n",
    "save(wArr,bVec)\n",
    "print(\"andel av bilder gjettet riktig: \" + str(test(wArr,bVec)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
